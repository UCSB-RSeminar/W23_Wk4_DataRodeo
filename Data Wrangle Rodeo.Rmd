---
title: "R Seminar - Tidying and Wrangling Data"
output: html_document
date: "2023-01-29"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# packages
library(tidyverse)
library(janitor)
library(lubridate)
library(readxl)
```
# *Welcome to the Data Rodeo!*
## Today we will learn how to wrangle data like true data cowpersons! We will start with a brief intro to the "tidyverse" and why us old-data-hands tend to stay in that universe. Then we will work through two feisty green datasets and whip them into shape!

![](wrangle.gif)

## The skills you should walk away from today with are: 
- creating reproducible and shareable results in the tidyverse 
- working with "pipes" 
- transforming dataframes into "long" format 
- pulling datasets apart and putting them back together again 
- ability to limit your dataset to just the rows/columns you want to work with 
- adding new columns to a dataframe 
 
# Part 0: What in Tarnation is the Tidyverse?

## In order to understand the **tidyverse**, we need to first understand what is meant by the term **tidy data**. The three principles of tidy data are that: 
### 1. Every **variable** is a column
### 2. Each **observation** is a row
### 3. Each cell contains a **single value**

```{r example tidy data, echo=FALSE}
plot <- rep(seq(1, 3, 1), 3)
date <- c("jan1", "jan1", "jan1", "jan2", "jan2", "jan2", "jan3", "jan3", "jan3") #has to be an easier way
count <- sample.int(100, 9) #random integers

longboi <- tibble(plot, date, count)
knitr::kable(longboi, align = "c")
```

### Tidy format is becoming a universal standard for data  
#### because it makes even the largest, unruliest dataset easy to manipulate and analyze. The paper that defined and popularized this concept, *Tidy Data* by Hadley Wickham (all hail), is available in this repo for you to peruse at will.

#### There is a suite of r tools and packages designed to work specifically with tidy data, and these are all contained in one mega-package, *tidyverse*. If you have not already done so, please install tidyverse now!

```{r eval=FALSE, message=FALSE, warning=FALSE, include=TRUE}
install.packages("tidyverse")
```

#### I won't harp too long on why tidyverse is amazing here, but if you're interested, the paper introducing the tidyverse to the world is also included in this repo.  
 
### With that brief intro, let's get started tidying!! 
 
# Part 1: Lasso Data from Wide to Long

#### Many data sheets that make it easy to record information in the field do so in "wide" format, meaning that each row has information about multiple "observations". Remember that tidy principles require only one observation per row, and each row having every piece of information relevant to that observation (variable values). Below are examples of wide format data:

### Wide Format Data Examples:
```{r wide boi, echo=FALSE}
plot <- seq(1,5,1)
jan1 <- c(23, 45, 78, 18, 19)
jan2 <- c(2, 5, 7, 8, 9)
jan3 <- c(34, 89, 89, 87, 21)

wideboi1 <- tibble(plot, jan1, jan2, jan3)
knitr::kable(wideboi1, align = "c")
```
```{r wideboi2, echo=FALSE}
ID <- c(1, 2, 3)
species <- c("F. parvipinnis", "Sebastes sp.", "S. pulcher")
stomach <- c(6, 18, 0)
intestine <- c(1, 0, 2)
gills <- c(3, 4, 7)

wideboi2 <- tibble(ID, species, stomach, intestine, gills)
knitr::kable(wideboi2, align = "c")
```


#### And, as a reminder, long format data looks like this:
```{r long boi, echo=FALSE}
plot <- rep(seq(1, 3, 1), 3)
date <- c("jan1", "jan1", "jan1", "jan2", "jan2", "jan2", "jan3", "jan3", "jan3") #has to be an easier way
count <- sample.int(100, 9) #random integers

longboi <- tibble(plot, date, count)
knitr::kable(longboi, align = "c")
```

### Our first task is to transform or "wrangle" our data from wide format to long format. 
#### Luckily this can be done in a few simple lines of code, depending on how messy the original dataset is. We are going to start with a dataset on tick survivorship at Tejon Ranch. 
 
In this study, ticks were put in mesh cages and buried. Every two weeks (or so) the live ticks in the cages were counted. 

```{r tick data import, echo=TRUE, message=FALSE, warning=FALSE}

## import our CSV
### make sure to use the tidyverse version of read_csv (with "_" instead of ".")

tick_wide <- read_csv("Data/tick18.csv") # I like to start by naming the dataframe "_wide" or "_unclean" so that the final dataframe gets a shorter name

tick_wide

```
#### When we load the data, we can see that there are several observations per row, one observation for each of several dates. Rows represent a single cage over time. 

#### To wrangle this data to a longer format, we will use a tidyverse function called "pivot_longer()". Pivot_longer() will let us make new columns for each variable, and put every observation in its own row.

#### Some other functions we will use and what they do: 
 
**%>%** - this is called a 'pipe' and it connects functions that are all relevant to a dataframe without having to call that dataframe multiple times. Think of it like a little magic link. A 'shortcut' to inserting a pipe is *command + shift + m* 
 
**mutate()** - mutate allows us to add a whole new column to the dataframe. This sounds pretty simple and limited, but its an incredibly powerful function. We will see what it can do throughout this tutorial. 
 
**filter()** - filter removes *rows* that match certain parameters that we give it, allowing us to filter out data we don't want (aka NAs, zeros, whatever) 
 
```{r pivoting longer}
tick_long <- tick_wide %>% 
  pivot_longer(names_to = "date_alive", # will turn current columns (specified below in 'cols =') into values in a new column called 'date alive'
               
               values_to = "num_alive", # will move the values in each column into a single column called 'num alive'. It will automatically match these values up with their respective date values
               
               cols = c("7/28/18" :"10/21/18"))%>% # specifies which columns to wrangle
  
  mutate(num_dead = lag(num_alive, default = first(num_alive))- num_alive) %>%  #creates a new column named 'num_dead' with values calculated using a function you don't need to worry about (if you are worried see below)

              # the 'lag' function that I've included here does a cool thing where it allows you to compute the                        difference between values in sequential rows, aka subtract the value of the second row from the first                  one to determine how many (in this example) ticks died between two time points.               
              #CHAT WITH ME AFTER CLASS IF YOU WANT TO LEARN MORE!  
  
  filter(num_dead >0) #remove rows with accidental negative values for num_dead (happens on the first day of the experiment)


tick_long
```
#### Now our data looks pretty squeaky clean! However, this unruly beastie of a dataset is **survival data** which actually requires that every single individual tick is a single observation, and, per tidy principles, has its own row. Luckily, there is A SINGLE LINE OF CODE that allows us to create such a data frame: uncount(). 

```{r sneaky further tidying}

tick_final <- uncount(tick_long, num_dead, #we will "uncount" to create a number of new rows based on the number in num_dead
                      .remove = FALSE) #remove = FALSE preserves the # dead, so we can check that uncount() did it right

knitr::kable(head(tick_final), align = 'c')
```

### Boy howdy! You have now successfully lassooed some pretty funky data! On to Part 2... 
  
# Part 2: Ride the Bucking Bronco That Is Sensor Data 
(Sorry Maddie for putting you on blast) 
 
#### In this experimental setup, we are taking fish heart rate data with a sensor that takes very messy data and exports it to an Excel sheet. 
We have two boxes (1 and 2) each with two channels (1 and 2 or 5 and 6) so four 'treatments' (box1_ch1, box1_ch2, box2_ch5, and box2_ch6). In each treatment there is a single fish.
 
We also have a separate CSV that has metadata for the fish, boxes, and channels that we will need to incorporate
 
#### Let's read in the two types of dataframes and clean up the metadata a bit... some new functions that we will use are: 
 
**select()** - love this function. Select lets us tell R which columns (variables) we want to keep. These are almost always variables we will use for analyses. Just helps us pare down our dataframe to just include elements we will use. 
 
**rename()** - pretty self explanatory; lets us rename column names 
 
**read_xlsx()**- I recently discovered the readxl package, and its a game changer for those of us who have massive datasets in Excel with multiple sheets and would rather not make a bunch of CSVs just for R. Lets you read in one sheet at a time. Whoever wrote this package gets me on a deep level. 
 
**head()** - allows us to view just the top few rows of a dataframe so we can check out its shape, columns, etc
 
```{r import fish data, message=FALSE, warning=FALSE}

metadata <- read_csv("Data/fish_metadata.csv") %>% 
  select(period_day, fish_ID, treatment) %>% # we are only interested in these variables/columns
  rename(fishID = fish_ID) # self explanatory I hope (I can't remember why I wanted to remove the underscore here but leaving it in because we're LEARNING)

fish_unclean <- read_xlsx("Data/fish.xlsx", sheet = "Sheet2") # read_xslx is truly magic, 'sheet =' allows you to pick which sheet you want

head(fish_unclean) # just lookin'

```

### Looks pretty crazy, but we can handle it! 
 
#### Our target dataframe will have these columns: fish, channel, box, start time, end time, time point, bpm, start temp, end temp 
 
For each individual fish, we need to know what channel it was in, box, start and end temperature, and its heart rate at each timepoint. This info is in separate data files and scattered around all caddywampus, so we are going to have to do some data-surgery. GRAB YOUR BONESAW. 
 
#### We will start by cleaning the fish data, removing all unnecessary columns, bad sensor runs, and NAs. 
 
#### Some other functions we will use and what they do: 

**clean_names()** - R hates spaces, slashes, capital letters, etc. clean_names() is in the *janitor* package and automaticall converts all column headers to "snake case", meaning all lowercase letters with no spaces or weird symbols (replaces these with underscores)
 
**c()** - this sneaky little combo creates a list for r using the elements, separated by commas, that you put within the parenthenses. Very flexible!
 
```{r cleaning fish data}

fish <- fish_unclean %>% 
  clean_names() %>% 
  rename(notes = 'notes_from_analysis_test') %>%  # this colname is TOO DAMN LONG
  select(!c(time, date, cmt_text, filename)) # ! in front of c() means select OUT the following list of columns


```

#### Now that the data is cleaner, we need to turn it into a long form (tidy) dataframe. This will include fish heartrates (beats per minute or "bpm") and all associated data that was taken for the whole test across all channels. To achieve this, we actually have to split up the dataframe and smash it back together. Bonesaws at the ready. 
 
```{r make a tidy heartrate dataset}
fish_hr <- fish %>% 
  select(ch1_bpm, ch2_bpm, ch5_bpm, ch6_bpm, 
         sel_start, sel_end, sel_duration, test_timepoint_start, test_timepoint_end, notes)  %>%  
  # only selecting for data that's taken when heart rate is taken (per timepoint) but for the entire test (aka not data taken in each channel like temperature)
  # we do need channel data so we will add it back in later once we've fixed this part of the dataframe
  
  # I want to get rid of the "_bpm" at the end of the channel columns since we won't need it
  rename_with( 
    # rename_with() tells r to rename columns using a function:
    ~str_remove(., "_bpm"), # remove the string "_bpm" from every column in this dataset (that's what the ".," means)
    ends_with("bpm")) %>%  # ...that ends with bpm

  # now we want to create one row per observation of bpm and create a col called channel so each bpm observation has a matched cell with the channel it was observed in
  
  # the rest of the data will just copy down appropriately

  pivot_longer(1:4, names_to = "channel", # column names for columns 1-4 will be put in a new col called channel
               values_to = "bpm") # observations will be put in a new col called bpm
```

#### Now we will making a separate long form dataframe of just the fish names in each channel, along with the data that was measured in each channel per fish trial (aka start and end temps) 
Remember than different channels had different treatments (aka different temperature regimes) 

```{r make a tidy dataframe of just data taken for each channel}

fish_ids  <- fish %>% 
  select(box1_ch1, box1_ch2, box2_ch5, box2_ch6, box1_temp_start, box1_temp_end, box2_temp_start, box2_temp_end) %>%  #only selecting names + temps (temps will copy + fill in appropriately)
  pivot_longer(1:4, names_to = c("box", "channel_copy"),
               names_sep = "_", 
               values_to = "fishID") %>% 
  mutate(fishID = case_when(fishID == 122 ~ "F122", 
                            TRUE ~ fishID))


```

#### The two separate dataframes that we've made are in the exact same order as right now so we can use a function called cbind() to smash them together relying on order of the rows. 

```{r data smash}
fish_full <- cbind(fish_ids, fish_hr) %>%  #only annoying thing about cbind is that it keeps all columns (both channel cols), but this lets us check for correct binding so it's ok
  select(!channel_copy) %>% 
  
  # we also want to smash the start and end temps into one column based on the box the fish was in
  # i.e. was fish in box 1? then we want start temp for box 1 in the col starttemp
  
  mutate(starttemp = if_else(box == "box1", box1_temp_start, box2_temp_start)) %>% 
  
  # if_else() says "if the value in column 'box' is 'box1', then paste in the matching value in column 'box1_temp_start'; otherwise, paste the value for box2_temp_start. NOTE: if we had more than two boxes this would get messy and we would likely have to use more complex wrangling methods!
  
  mutate(endtemp = if_else(box == "box1", box1_temp_end, box2_temp_end)) # same deal as above but for end temps
 
fish_full$starttemp <- as.numeric(fish_full$starttemp) #sometimes R loses sight of the fact that values are numbers and we need to remind it using as.numeric()
fish_full$endtemp <- as.numeric(fish_full$endtemp)

```
#### Our final act of wrangling involves appending the "metadata" that we pulled in earlier, which has more information about experimental runs and individual fish that we might want. Luckily this data is already in tidy format, so we can just merge it with 'fish_full' using the join() function (specifically left_join()): 
```{r final smash!}
fish_final <- fish_full %>% 
  select(fishID, bpm, starttemp, endtemp,channel, test_timepoint_start, test_timepoint_end, notes)%>% 
  filter(!is.na(starttemp), 
         fishID != "NA") %>% #this removes NAs. In our case, fish called "NA" were unsuccessful runs, so we want them out of the dataset
  left_join(., metadata, by="fishID") #left_join() smashes our two datasets together, prioritizing the information in the "left" dataset, aka 'fish_full'. We told R to matche the two datasets using the column "fishID".

fish_final
```
### Ain't she a beaut.

# Congratulations, you are officially a bonified DATA WRANGLER!!

